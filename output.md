# Aggregator node of an EFK setup

This section provides the overview and installation/creation of an aggregator node of an EFK setup.

## Contents
1. [Overview](#overview)
    1. [Collector](#collector)
    1. [Aggregator](#aggregator)

2. [Aggregator](#aggregator-1)
    1. [Fluentd](#fluentd)
    2. [Elasticsearch](#elasticsearch)
    3. [Kibana](#kibana)

3. [Setting it up](#setting-it-up)
    1. [Install and configure aggregator](#install-and-configure-aggregator)
       1. [Pre-installation](#pre-installation)
       2. [Installation](#installation)
       3. [td-agent setup](#td-agent-setup)
       4. [Configuration](#configuration)

## Overview
The EFK ([Elasticsearch](https://www.elastic.co/products/elasticsearch), [Fluentd](http://www.fluentd.org/), [Kibana](https://www.elastic.co/products/kibana)) setup is a setup to gather, index and visualize logs.

The setup consists of two kind of nodes.
- Collector
- Aggregator

#### Collector
Collector nodes are the nodes in the cluster that collect logs from the local system, parse them, and forward them to the aggregator node(s).<br><br>
The collector nodes usually are the same as all the *working nodes* of the cluster. By *working nodes*, I mean the nodes that are running the services, processes and carrying out the tasks of which you need to collect the logs. The collector in those nodes collects the logs generated by the processes, parses them in a meaningful structure (mostly [JSON](http://www.json.org/)), and forwards them to the aggregator node(s).

#### Aggregator
Aggregator nodes are nodes that receive the logs (in JSON format), index them in Elasticsearch, and visualize them on Kibana.<br><br>
The aggregator nodes do not necessarily have to exist outside the *working nodes* of the cluster, but are preferred that way, since if ever the working nodes get faulty (or crash), the aggregator nodes are not affected and can be used to trace the cause to the error.<br>
This document talks about the aggregator nodes.

## Aggregator
The aggregator node broadly has three tasks.

1. Receive parsed logs from the collector nodes.
2. Index the logs in Elasticsearch.
3. Visualize the logs in Kibana.

We use three different software components to accomplish the three tasks.

1. [Fluentd](http://fluentd.org/) - Receive the parsed logs.
2. [Elasticsearch](https://www.elastic.co/products/elasticsearch) - Index the logs in a searchable format.
3. [Kibana](https://www.elastic.co/products/kibana) - Visualize the logs in charts and graphs.

#### Fluentd
**_Fluentd is an open source data collector for unified logging layer._**<br><br>
Fluentd is a pluggable open source tool that collects, parses and transports logs between hosts. It is written in [Ruby](https://www.ruby-lang.org/en/), and is backed commercially by [Treasure Data](https://www.treasuredata.com/).
We are using [td-agent](https://docs.treasuredata.com/articles/td-agent), the stable distribution package of Fluentd, QAed by Treasure Data.<br>
td-agent is run as a service, and reads a [configuration file](http://docs.fluentd.org/articles/config-file) located at `/etc/td-agent/td-agent.conf` to control its input and output behaviour, by
 - selecting input and output plugins and,
 - specifying the plugin parameters.

The file is required for Fluentd to operate properly.

Fluentd is also used in the collector nodes to collect and forward the logs.


#### Elasticsearch
**_Elasticsearch is a highly scalable open-source full-text search and analytics engine._**<br><br>
Elasticsearch is used for full-text search, structured search, analytics and all three in combination. It is built on top of [Apache Lucene](https://lucene.apache.org/), a full-text search-engine library. Written in Java, it uses Lucene internally for all its indexing and searching. It provides a simple and coherent RESTful API.

Elasticsearch is distributed in nature, running on multiple nodes in a cluster, providing replication and sharding for improved performance and redundancy. Elasticsearch is configured using its configuration files, located at `.../config/`.


#### Kibana
**_Kibana is an open source analytics and visualization platform designed to work with Elasticsearch._**<br><br>
Kibana is used as a visualization tool, on top of Elasticsearch. It provides real-time analytics on data in Elasticsearch.

Kibana dashboard can be accessed on a web browser. It can be configured via its config file at `.../config/kibana.yml`.


## Setting it up
The aggregator node can be setup via two methods.

1. Install and configure the components on an existing machine.
2. Spawn a VM from an aggregator image.

Since the first method can also be used to create a VM image for use in the second method, we'll discuss the first method only.

### Install and configure aggregator
The installation of software components of aggregator can be divided into four phases.
- Pre-installation
- Installation
- td-agent setup
- Configuration

#### Pre-installation
The pre-installation consists of setting up the environment for installing EFK, and obtaining all the packages.

During pre-installation, we'll install
- **curl**: We'll use `curl` to download the software packages from the internet (or local repository).
- **unzip**: We'll use `unzip` to extract elasticsearch plugins and install them.
- **ntp**: We'll use ntp to synchronize the clocks bewteen the interacting hosts.
- **java**: A JVM is required by elasticsearch for execution.

After installation of these tools, we will obtain all the packages. The packages to be downloaded:
- [Elasticsearch 1.72](https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.2.tar.gz)
- [Elasticsearch Head plugin](https://codeload.github.com/mobz/elasticsearch-head/zip/master)
- td-agent installation [script](https://toolbelt.treasuredata.com/sh/install-ubuntu-trusty-td-agent2.sh)
- [Kibana 4.1.2](https://download.elastic.co/kibana/kibana/kibana-4.1.2-linux-x64.tar.gz)

We also increase the maximum number of file descriptors from 1024 (default) to 65536.

#### Installation
During the installation phase, we install the downloaded packages in their appropriate locations.

The elasticsearch and kibana tarballs are to be extracted to `/opt/`, or any other appropriate location.<br>
The elasticsearch-head-plugin file downloaded is actually a zip file, and needs to be extracted to `/opt/elasticsearch-1.7.2/plugins/` (and renamed to `head`).

For td-agent, installation from the downloaded script is the most recommended method. Run the script to install td-agent.


#### td-agent setup
After installing td-agent, we need to install its gems that are required for our use case.
We need to install the following gems:
- fluent-plugin-elasticsearch
- fluent-plugin-forest

It is recommended to remove one json gem to avoid errors and warnings as td-agent (strangely) comes with two json plugins, of different versions.

#### Configuration
After installation, we need to configure the softwares as per our requirement.

The files required for proper configuration, or reference can be found [here](https://github.com/safiyat/fluentd-openstack-logging/tree/ee5ec5588d6bf54bd35c376debed241f32c94afc).

The configuration files to be modified/created are:

1. `td-agent.conf`

    This file contains the configuration for the `td-agent` service running on the aggregator node.
    This file configures `td-agent` daemon to listen for the forwarding `td-agent` on `0.0.0.0:24220`. It also dynamically sets the index and type of the log to be pushed in elasticsearch.
    <br>
    Copy it to the location `/etc/td-agent/`.

2. `aggregator.conf`

    This file is an upstart script, to start the necessary EFK services on the aggregator upon boot.
    <br>
    Copy it to the location `/etc/init/`.

3. `elastic/`

    1. `elasticsearch.yml`

        This file contains the configuration for the elasticsearch daemon running on the aggregator.
        It sets the cluster name to **sdcloud**. It also sets the path to store elasticsearch logs and sets the IPs to be unicasted for discovery.
        <br>
        Copy it to the location `/opt/elasticsearch-1.7.2/config/`.

    2. `logging.yml`
        This file contains the logging configuration for the elasticsearch daemon running on the aggregator.
        <br>
        Copy it to the location `/opt/elasticsearch-1.7.2/config/`.

4. `kibana.yml`

    This file contains the configuration for kibana running on the aggregator node.
    <br>
    Copy it to the location `/opt/kibana-4.1.2-linux-x64/config/`.

In addition to the above mentioned configurations, we also need to set the mappings of fields of type `string` in elasticsearch to [`not_analyzed`](https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping-intro.html#custom-field-mappings), using a curl request (or utilising [this script](https://github.com/safiyat/EFKsetup/blob/02b73879d7a7b6698e1fdb15766246f232402862/Aggregator/set_not_analyzed.sh).)










## ISSUES
- ~~Fluentd flushes to elasticsearch with an impatient delay.~~

## To be done
- Set up log rotation. Use cronjob to delete all logs older than 30 days and all logs (except `loglevel:ERROR`) older than 7 days.
- Use elasticsearch and kibana from apt-get repo instead of tarballs.
- Setup multiline suppport for log parsing. Look [here](https://review.openstack.org/cat/114485%2C1%2Ctools/ansible-openstack-log/templates/etc/td-agent/td_agent.conf%5E0).
- Rabbitmq and Mysql log config.
- Parse the request tokens into request id, user id, and tenant id. Look [here](https://review.openstack.org/cat/114485%2C1%2Ctools/ansible-openstack-log/templates/etc/td-agent/td_agent.conf%5E0).
- Fix the analyzed string issue in kibana. Especially for the host names.
- Read and implement [this](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html).
- Parse the log file `/var/log/apache2/keystone.log`
- ~~Parse the nova-api.log file properly. Skip the stack trace.~~
- ~~Categorize the logs in their various indices.~~
- ~~Write all the configs and include them in the `collector.conf`~~

## Configuration
### Aggregator
- Fluentd
  - Log level: TRACE
  - IP: 0.0.0.0 (Default is 0.0.0.0)
  - Port: 24220 (Default is 24224)

- Elasticsearch
  - Clustername: sdcloud
  - IP: 0.0.0.0 (Default is 0.0.0.0)
  - Port: 9200 (Default is 9200)
  - Flush interval: 1s
  - Index name: Second tag part
  - Type name: Third tag part

### Collector
- Connet
  - Forward to
    - Active: 10.41.3.229:24220, 10.41.3.230:24220
    - Standby:

- Compute
  - Forward to
    - Active: 10.41.3.229:24220, 10.41.3.230:24220
    - Standby:

## Used plugins
- [Elasticsearch Plugin](https://github.com/uken/fluent-plugin-elasticsearch)
- [Grok Parser](https://github.com/kiyoto/fluent-plugin-grok-parser)
- [Forest Plugin](https://github.com/tagomoris/fluent-plugin-forest)
- [Retag Plugin](https://github.com/algas/fluent-plugin-retag)
